{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#5fa8d3\"> Yolo model\n",
    "\n",
    "\n",
    "## <font color=\"#62b6cb\"> Table of Content <a name=\"ToC\"></a>\n",
    "0. [Libraries Importation, Parameters & Funtions Definition](#id0)<br>\n",
    "1. [Preparation of the data](#id1)<br>\n",
    "    1.1 [Copy Images to Yolo Folder](#id11)<br>\n",
    "    1.2 [Obtain the labels for YOLO](#id12)<br>\n",
    "2. [Yolo Model Trainings in Sagemaker](#id2)<br>\n",
    "    2.1 [Include Credentials for Sagemaker](#id21)<br>\n",
    "    2.2 [Train a single model in Sagemaker](#id22)<br>\n",
    "    2.3 [Hyperparameter tuning in Sagemaker](#id23)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;2.3.1 [Hyperparameters space and tuning code for Sagemaker](#id231)<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;2.3.2 [Select best tuning model and metrics](#id232)<br>\n",
    "3. [Train with the Final Model](#id3)<br>\n",
    "    3.1 [Train and Validation Datasets Union](#id31)<br>\n",
    "    3.2 [Retrain the Final Model in Sagemaker](#id32)<br>\n",
    "4. [Generalization Check](#id4)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#62b6cb\"> 0. Libraries Importation, Parameters & Funtions Definition <a name=\"id0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Notebooks/data.yaml'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# things to put in the config\n",
    "'./Notebooks/data.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T15:40:57.002183Z",
     "start_time": "2024-07-25T15:40:56.937474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "import yaml # for importing a yaml file\n",
    "import joblib\n",
    "import torch\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import boto3\n",
    "import tarfile\n",
    "import joblib\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (CategoricalParameter, ContinuousParameter, IntegerParameter, HyperparameterTuner, \n",
    "                            HyperbandStrategyConfig, StrategyConfig, HyperparameterTuningJobAnalytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: /home/sagemaker-user/project-danielteresa\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "last_folder = os.path.basename(current_directory)\n",
    "    \n",
    "if last_folder != \"project-danielteresa\":\n",
    "    while last_folder != \"project-danielteresa\":\n",
    "        parent_directory = os.path.dirname(current_directory)\n",
    "        last_folder = os.path.basename(parent_directory)\n",
    "\n",
    "        os.chdir(parent_directory)\n",
    "        print(f\"Changed directory to: {parent_directory}\")\n",
    "else:\n",
    "    print(\"Already in the project root directory.\")\n",
    "\n",
    "# our modules\n",
    "from src.mymodule import * # for importing our functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables set from YAML file:\n",
      "seed: 123\n",
      "color1: #62b6cb\n",
      "color2: #fb8500\n",
      "color3: #023047\n",
      "color4: #FFB703\n",
      "path_annotations: ./Data\n",
      "path_train: ./Data/train/original\n",
      "path_train_train: ./Data/train/train\n",
      "path_train_aug: ./Data/train/train_aug\n",
      "path_train_val: ./Data/train/val\n",
      "path_test: ./Data/test\n",
      "path_yolo: ./Data/Yoloimages/\n",
      "path_models: ./Models\n",
      "kaggle_train_annotations: annotations_train.json\n",
      "train_annotations_name_temp: annotations_train_temp.json\n",
      "train_annotations_name: annotations_train_updated.json\n",
      "aug_train_annotations_name: annotations_train_updated_aug.json\n",
      "val_annotations_name: annotations_val_updated.json\n",
      "kaggle_test_annotations: annotations_test.json\n",
      "test_annotations_name_temp: annotations_test_temp.json\n",
      "test_annotations_name: annotations_test_updated.json\n",
      "weights_yolo_path: Models/yolo_weights\n",
      "runs_path: Models/runs\n",
      "bucket_name: sagemaker-eu-west-1-project-danielteresa/\n",
      "bucket_name2: sagemaker-eu-west-1-project-danielteresa\n"
     ]
    }
   ],
   "source": [
    "# Load the YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Function to set variables globally and store their names\n",
    "def set_variables(config, prefix='', var_dict={}):\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            set_variables(value, prefix + key + '_', var_dict)\n",
    "        else:\n",
    "            globals()[prefix + key] = value\n",
    "            var_dict[prefix + key] = value\n",
    "    return var_dict\n",
    "\n",
    "# Set variables globally and get a dictionary of the set variables\n",
    "set_vars = set_variables(config)\n",
    "\n",
    "# Print all the variables that were set\n",
    "print(\"Variables set from YAML file:\")\n",
    "for var_name, var_value in set_vars.items():\n",
    "    print(f\"{var_name}: {var_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#62b6cb\"> 1. Preparation of data <a name=\"id1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 1.1 Copy Images to Yolo Folder  <a name=\"id11\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/train/train_aug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to delete \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Reason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Copy images from train augmented to Yolo train folder\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_train_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_yolo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain/images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopytree(path_train_val, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_yolo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval/images\u001b[39m\u001b[38;5;124m\"\u001b[39m), dirs_exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopytree(path_test, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_yolo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest/images\u001b[39m\u001b[38;5;124m\"\u001b[39m), dirs_exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:557\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Recursively copy a directory tree and return the destination directory.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03mIf exception(s) occur, an Error is raised with a list of reasons.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m`src` tree.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutil.copytree\u001b[39m\u001b[38;5;124m\"\u001b[39m, src, dst)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[1;32m    558\u001b[0m     entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _copytree(entries\u001b[38;5;241m=\u001b[39mentries, src\u001b[38;5;241m=\u001b[39msrc, dst\u001b[38;5;241m=\u001b[39mdst, symlinks\u001b[38;5;241m=\u001b[39msymlinks,\n\u001b[1;32m    560\u001b[0m                  ignore\u001b[38;5;241m=\u001b[39mignore, copy_function\u001b[38;5;241m=\u001b[39mcopy_function,\n\u001b[1;32m    561\u001b[0m                  ignore_dangling_symlinks\u001b[38;5;241m=\u001b[39mignore_dangling_symlinks,\n\u001b[1;32m    562\u001b[0m                  dirs_exist_ok\u001b[38;5;241m=\u001b[39mdirs_exist_ok)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/train/train_aug'"
     ]
    }
   ],
   "source": [
    "# Create the folders if they don't exist\n",
    "directories = [\n",
    "    os.path.join(path_yolo, \"train/images\"),\n",
    "    os.path.join(path_yolo, \"train/labels\"),\n",
    "    os.path.join(path_yolo, \"val/images\"),\n",
    "    os.path.join(path_yolo, \"val/labels\"),\n",
    "    os.path.join(path_yolo, \"test/images\"),\n",
    "    os.path.join(path_yolo, \"test/labels\")\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Remove everything that exists in the folders\n",
    "for directory in [    os.path.join(path_yolo, \"train/images\"),\n",
    "                      os.path.join(path_yolo, \"train/labels\"),\n",
    "                      os.path.join(path_yolo, \"val/images\"),\n",
    "                      os.path.join(path_yolo, \"val/labels\"),\n",
    "                      os.path.join(path_yolo, \"test/images\"),\n",
    "                      os.path.join(path_yolo, \"test/labels\")]:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove the file\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # Remove the directory and its contents\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "# Copy images from train augmented to Yolo train folder\n",
    "shutil.copytree(path_train_aug, os.path.join(path_yolo, \"train/images\"), dirs_exist_ok=True)\n",
    "shutil.copytree(path_train_val, os.path.join(path_yolo, \"val/images\"), dirs_exist_ok=True)\n",
    "shutil.copytree(path_test, os.path.join(path_yolo, \"test/images\"), dirs_exist_ok=True)\n",
    "\n",
    "print(\"Folders have been cleaned and files have been copied successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the annotations and id of the coco jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that annotation for training is correct\n",
      "True\n",
      "Check that annotation for validation is correct\n",
      "True\n",
      "Check that annotation for test is correct\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that annotation for training is correct\")\n",
    "print(validate_coco_dataset(os.path.join(path_annotations,aug_train_annotations_name), os.path.join(path_yolo,\"train/images\")))\n",
    "\n",
    "print(\"Check that annotation for validation is correct\")\n",
    "print(validate_coco_dataset(os.path.join(path_annotations,val_annotations_name), os.path.join(path_yolo,\"val/images\")))\n",
    "\n",
    "print(\"Check that annotation for test is correct\")\n",
    "print(validate_coco_dataset(os.path.join(path_annotations,test_annotations_name), os.path.join(path_yolo,\"test/images\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the points of the polygons are out of the range of the width and the height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the points of the polygons are out of the range of the images\n",
    "\n",
    "# train\n",
    "process_coco_annotations(os.path.join(path_annotations,aug_train_annotations_name),\n",
    "                          os.path.join(path_annotations,aug_train_annotations_name))\n",
    "\n",
    "# val\n",
    "process_coco_annotations(os.path.join(os.path.join(path_annotations,val_annotations_name)), \n",
    "                         os.path.join(os.path.join(path_annotations,val_annotations_name)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width range: 204 to 2365\n",
      "Height range: 153 to 2560\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(path_annotations, train_annotations_name), 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract image sizes\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for image in coco_data['images']:\n",
    "    widths.append(image['width'])\n",
    "    heights.append(image['height'])\n",
    "\n",
    "# Find the range of widths and heights\n",
    "min_width = min(widths)\n",
    "max_width = max(widths)\n",
    "min_height = min(heights)\n",
    "max_height = max(heights)\n",
    "\n",
    "print(f\"Width range: {min_width} to {max_width}\")\n",
    "print(f\"Height range: {min_height} to {max_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 1.2 Obtain the labels for YOLO  <a name=\"id12\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the yolo txt for each images from the coco data annotations using the conver_coco from ultralytics. The annotations json to convert should be in a folder with that json in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/sagemaker-user/project-danielteresa/Data/Yoloimages/train/annotations_train_updated_aug.json: 100%|██████████| 46485/46485 [00:21<00:00, 2120.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/sagemaker-user/project-danielteresa/Data/Yoloimages/aux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed auxiliary directory ./Data/Yoloimages/aux\n",
      "Yolo labels saved in ./Data/Yoloimages/train/labels\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/sagemaker-user/project-danielteresa/Data/Yoloimages/val/annotations_val_updated.json: 100%|██████████| 2324/2324 [00:00<00:00, 11731.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/sagemaker-user/project-danielteresa/Data/Yoloimages/aux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed auxiliary directory ./Data/Yoloimages/aux\n",
      "Yolo labels saved in ./Data/Yoloimages/val/labels\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/sagemaker-user/project-danielteresa/Data/Yoloimages/test/annotations_test_updated.json: 100%|██████████| 2324/2324 [00:00<00:00, 12472.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/sagemaker-user/project-danielteresa/Data/Yoloimages/aux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed auxiliary directory ./Data/Yoloimages/aux\n",
      "Yolo labels saved in ./Data/Yoloimages/test/labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain the yolo labels\n",
    "# train\n",
    "convert_coco_to_yolo_segmentation(path_annotations, aug_train_annotations_name, path_yolo, 'train')\n",
    "# val\n",
    "convert_coco_to_yolo_segmentation(path_annotations, val_annotations_name, path_yolo, 'val')\n",
    "# test\n",
    "convert_coco_to_yolo_segmentation(path_annotations, test_annotations_name, path_yolo, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the nº of elememts in the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in './Data/Yoloimages//train/labels': 46485\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "directory_path = path_yolo +'/train/labels'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Count the number of files\n",
    "file_count = len([file for file in files if os.path.isfile(os.path.join(directory_path, file))])\n",
    "\n",
    "print(f\"Number of files in '{directory_path}': {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#62b6cb\"> 2. Yolo Model Trainings in Sagemaker  <a name=\"id2\"></a> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the files to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Data/Yoloimages/train: 100%|██████████| 92970/92970 [09:48<00:00, 157.96file/s]\n",
      "Uploading Data/Yoloimages/val: 100%|██████████| 4648/4648 [00:26<00:00, 173.96file/s]\n",
      "Uploading Data/Yoloimages/test: 100%|██████████| 4649/4649 [00:26<00:00, 174.29file/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the bucket name and the folder to copy to S3\n",
    "bucket_train = \"sagemaker-eu-west-1-project-danielteresa/train\"\n",
    "local_train = \"Data/Yoloimages/train\"\n",
    "\n",
    "bucket_val = \"sagemaker-eu-west-1-project-danielteresa/val\"\n",
    "local_val = \"Data/Yoloimages/val\"\n",
    "\n",
    "bucket_test = \"sagemaker-eu-west-1-project-danielteresa/test\"\n",
    "local_test = \"Data/Yoloimages/test\"\n",
    "\n",
    "# Upload train, test and validation folders\n",
    "upload_folder_to_s3(local_train, bucket_train)\n",
    "upload_folder_to_s3(local_val, bucket_val)\n",
    "upload_folder_to_s3(local_test, bucket_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we upload the annotations the data.yaml file for train the model too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_annotations,val_annotations_name), 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "names = [class_name['name'] for class_name in coco_data[\"categories\"]]\n",
    "\n",
    "# Specify the paths and information\n",
    "\n",
    "train_path = 'train_prueba/images'\n",
    "path_yolo_val = 'val_prueba/images'\n",
    "\n",
    "names_categories = [class_name['name'] for class_name in coco_data[\"categories\"]]\n",
    "nc = len(names)\n",
    "file_path = './Notebooks/data.yaml'\n",
    "\n",
    "# Create the YAML file\n",
    "create_yaml_file(file_path, train_path, path_yolo_val, nc, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded ./Notebooks/data.yaml to s3://sagemaker-eu-west-1-project-danielteresa/data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Define the bucket name and the file to copy to S3\n",
    "bucket_annotations = \"sagemaker-eu-west-1-project-danielteresa\"\n",
    "file_to_upload = \"./Notebooks/data.yaml\"\n",
    "\n",
    "# Upload the file to the specified S3 bucket\n",
    "upload_file_to_s3(file_to_upload, bucket_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already create the prueba images and upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Set up\n",
    "train_folder = './Data/Yoloimages/train'\n",
    "val_folder = './Data/Yoloimages/val'\n",
    "\n",
    "destination_train_folder = './Data/Yoloimages/train_prueba'\n",
    "destination_val_folder = './Data/Yoloimages/val_prueba'\n",
    "\n",
    "# Seleccionar 20 imágenes de cada carpeta\n",
    "select_images(train_folder, destination_train_folder, 20, 'images', 'labels', seed)\n",
    "select_images(val_folder, destination_val_folder, 20, 'images', 'labels', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Data/Yoloimages/train_prueba: 100%|██████████| 40/40 [00:00<00:00, 50.95file/s]\n",
      "Uploading Data/Yoloimages/val_prueba: 100%|██████████| 40/40 [00:00<00:00, 51.80file/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the bucket name and the folder to copy to S3\n",
    "bucket_train = \"sagemaker-eu-west-1-project-danielteresa/train_prueba\"\n",
    "local_train = \"Data/Yoloimages/train_prueba\"\n",
    "\n",
    "bucket_val = \"sagemaker-eu-west-1-project-danielteresa/val_prueba\"\n",
    "local_val = \"Data/Yoloimages/val_prueba\"\n",
    "\n",
    "\n",
    "# Upload train, test and validation folders\n",
    "upload_folder_to_s3(local_train, bucket_train)\n",
    "upload_folder_to_s3(local_val, bucket_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 2.1. Include Credentials for Sagemaker  <a name=\"id21\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the credentials to run the trainning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ADD HERE CREDENTIALS\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "role_arn = 'arn:aws:iam::794367255496:role/daniel.quesada10'  # Replace with your actual role ARN\n",
    "\n",
    "assumed_role = sts_client.assume_role(\n",
    "    RoleArn=role_arn,\n",
    "    RoleSessionName='SageMakerSession'\n",
    ")\n",
    "\n",
    "credentials = assumed_role['Credentials']\n",
    "\n",
    "# Use the assumed role credentials to create a new session\n",
    "sagemaker_session = sagemaker.Session(\n",
    "    boto3.Session(\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 2.2 Train a single model in Sagemaker  <a name=\"id22\"></a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce the train py file for running the train job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install ultralytics\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters\n",
    "    # parser.add_argument(\"--num_images\", type=int, default=50)\n",
    "    # parser.add_argument(\"--seed\", type=int, default=123)\n",
    "    \n",
    "    parser.add_argument('--epochs',type=int, help='number of training epochs')\n",
    "    parser.add_argument(\"--batch\", type=int, default=5)\n",
    "    \n",
    "    parser.add_argument('--optimizer', type=str, help='optimizer to use')\n",
    "    parser.add_argument('--lr0', type=float, help='initial learning rate')\n",
    "    parser.add_argument('--lrf', type=float, help='final learning rate')\n",
    "    parser.add_argument('--momentum', type=float, help='momentum')\n",
    "    parser.add_argument('--weight_decay', type=float, help='optimizer weight decay')\n",
    "\n",
    "    # SageMaker specific arguments\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--runs-path\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "\n",
    "   \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('---------------Debug injected environment and arguments--------------------')\n",
    "    print(sys.argv)\n",
    "    print(os.environ)\n",
    "    print('---------------End debug----------------------')\n",
    "\n",
    "  \n",
    "    # Train the YOLO model\n",
    "    # yolo_model = YOLO(os.path.join(args.weights_yolo_path, \"yolov8m-seg.pt\"))\n",
    "    yolo_model = YOLO(\"yolov8m-seg.pt\")\n",
    "    yolo_model.train(data=os.path.join(args.train, \"data.yaml\"), \n",
    "                     batch=args.batch,\n",
    "                     epochs=args.epochs, \n",
    "                     optimizer=args.optimizer, \n",
    "                     lr0=args.lr0, \n",
    "                     lrf=args.lrf, \n",
    "                     momentum=args.momentum,\n",
    "                     weight_decay=args.weight_decay,\n",
    "                     task='segment',\n",
    "                     project=args.runs_path)\n",
    "    \n",
    "    yolo_model.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally train in sagemaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework='pytorch',\n",
    "    region='eu-west-1',\n",
    "    version='1.12.1',\n",
    "    py_version='py38',\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    image_scope='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"precision\", \"Regex\": \"YOLO Metric metrics/precision\\\\(B\\\\): (.*)\"},\n",
    "    {\"Name\": \"recall\", \"Regex\": \"YOLO Metric metrics/recall\\\\(B\\\\): (.*)\"},\n",
    "    {\"Name\": \"mAP50\", \"Regex\": \"YOLO Metric metrics/mAP50\\\\(B\\\\): (.*)\"},\n",
    "    {\"Name\": \"mAP50-95\", \"Regex\": \"YOLO Metric metrics/mAP50-95\\\\(B\\\\): (.*)\"},\n",
    "    {\"Name\": \"box_loss\", \"Regex\": \"YOLO Metric val/box_loss: (.*)\"},\n",
    "    {\"Name\": \"cls_loss\", \"Regex\": \"YOLO Metric val/cls_loss: (.*)\"},\n",
    "    {\"Name\": \"dfl_loss\", \"Regex\": \"YOLO Metric val/dfl_loss: (.*)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-08-03-08-36-23-343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-03 08:36:23 Starting - Starting the training job......\n",
      "2024-08-03 08:37:06 Starting - Preparing the instances for training...\n",
      "2024-08-03 08:37:27 Downloading - Downloading input data............................................................\n",
      "2024-08-03 08:47:48 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,687 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,689 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,691 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,702 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,705 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,971 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,973 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,988 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:49,990 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:50,006 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:50,009 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:50,022 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"lr0\": 0.01,\n",
      "        \"lrf\": 0.01,\n",
      "        \"momentum\": 0.937,\n",
      "        \"optimizer\": \"Adam\",\n",
      "        \"weight_decay\": 0.0005\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-08-03-08-36-23-343\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-project-danielteresa/pytorch-training-2024-08-03-08-36-23-343/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"lr0\":0.01,\"lrf\":0.01,\"momentum\":0.937,\"optimizer\":\"Adam\",\"weight_decay\":0.0005}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-project-danielteresa/pytorch-training-2024-08-03-08-36-23-343/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"lr0\":0.01,\"lrf\":0.01,\"momentum\":0.937,\"optimizer\":\"Adam\",\"weight_decay\":0.0005},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2024-08-03-08-36-23-343\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-project-danielteresa/pytorch-training-2024-08-03-08-36-23-343/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--lr0\",\"0.01\",\"--lrf\",\"0.01\",\"--momentum\",\"0.937\",\"--optimizer\",\"Adam\",\"--weight_decay\",\"0.0005\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR0=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_LRF=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_MOMENTUM=0.937\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=Adam\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0005\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --epochs 10 --lr0 0.01 --lrf 0.01 --momentum 0.937 --optimizer Adam --weight_decay 0.0005\u001b[0m\n",
      "\u001b[34m2024-08-03 08:47:52,591 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCollecting ultralytics\u001b[0m\n",
      "\u001b[34mDownloading ultralytics-8.2.71-py3-none-any.whl (863 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 863.7/863.7 kB 22.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (4.7.0.68)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (0.13.1+cu113)\u001b[0m\n",
      "\u001b[34mCollecting ultralytics-thop>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (1.12.1+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (3.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (0.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ultralytics) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.8/site-packages (from ultralytics) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.8/site-packages (from ultralytics) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (5.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ultralytics-thop, ultralytics\u001b[0m\n",
      "\u001b[34mSuccessfully installed ultralytics-8.2.71 ultralytics-thop-2.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m---------------Debug injected environment and arguments--------------------\u001b[0m\n",
      "\u001b[34m['train.py', '--epochs', '10', '--lr0', '0.01', '--lrf', '0.01', '--momentum', '0.937', '--optimizer', 'Adam', '--weight_decay', '0.0005']\u001b[0m\n",
      "\u001b[34menviron({'SM_INPUT_DIR': '/opt/ml/input', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-3', 'NCCL_DEBUG': 'WARN', 'HOROVOD_VERSION': '0.24.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_CURRENT_INSTANCE_TYPE': 'ml.m5.2xlarge', 'SM_HP_OPTIMIZER': 'Adam', 'EFA_VERSION': '1.15.1', 'MASTER_ADDR': 'algo-1', 'SM_MODULE_NAME': 'train', 'HOSTNAME': 'ip-10-0-239-227.eu-west-1.compute.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.8/site-packages/cv2/../../lib64:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib', 'HOME': '/root', 'MASTER_PORT': '7777', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:eu-west-1:794367255496:training-job/pytorch-training-2024-08-03-08-36-23-343', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_HP_LR0': '0.01', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'BRANCH_OFI': '1.3.0-aws', 'SM_HP_WEIGHT_DECAY': '0.0005', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-be43d8dda097835816e2ecb8821e269f3a650a8a2b11db55da9e81868f90c783-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '8', 'CUDA_VERSION': '11.3.1', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'SM_NUM_GPUS': '0', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_NUM_NEURONS': '0', 'SM_MODULE_DIR': 's3://sagemaker-eu-west-1-project-danielteresa/pytorch-training-2024-08-03-08-36-23-343/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.3 brand=tesla,driver>=418,driver<419 driver>=450', 'RDMAV_FORK_SAFE': '1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SM_LOG_LEVEL': '20', 'SM_HP_EPOCHS': '10', 'NV_CUDA_CUDART_VERSION': '11.3.109-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"lr0\":0.01,\"lrf\":0.01,\"momentum\":0.937,\"optimizer\":\"Adam\",\"weight_decay\":0.0005},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2024-08-03-08-36-23-343\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-project-danielteresa/pytorch-training-2024-08-03-08-36-23-343/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}', 'PATH': '/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'DMLC_INTERFACE': 'eth0', 'SM_INSTANCE_GROUPS_DICT': '{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}}', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'SM_HPS': '{\"epochs\":10,\"lr0\":0.01,\"lrf\":0.01,\"momentum\":0.937,\"optimizer\":\"Adam\",\"weight_decay\":0.0005}', 'NCCL_IB_DISABLE': '1', 'SM_INSTANCE_GROUPS': '[\"homogeneousCluster\"]', 'CUDNN_VERSION': '8.2.0.53', 'DEBIAN_FRONTEND': 'noninteractive', 'SM_CHANNELS': '[\"train\"]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_CURRENT_INSTANCE_GROUP_HOSTS': '[\"algo-1\"]', 'SM_IS_HETERO': 'false', 'AWS_REGION': 'eu-west-1', 'OPEN_MPI_PATH': '/opt/amazon/openmpi', 'DLC_CONTAINER_TYPE': 'training', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'SAGEMAKER_JOB_NAME': 'pytorch-training-2024-08-03-08-36-23-343', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train', 'SM_HP_LRF': '0.01', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'pytorch-training-2024-08-03-08-36-23-343', 'LC_ALL': 'C.UTF-8', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_VERSION': '4.1.1', 'SM_HOSTS': '[\"algo-1\"]', 'SM_DISTRIBUTION_INSTANCE_GROUPS': '[]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'eu-west-1', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.10.3', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--epochs\",\"10\",\"--lr0\",\"0.01\",\"--lrf\",\"0.01\",\"--momentum\",\"0.937\",\"--optimizer\",\"Adam\",\"--weight_decay\",\"0.0005\"]', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'SM_CURRENT_INSTANCE_GROUP': 'homogeneousCluster', 'SM_HP_MOMENTUM': '0.937', 'OMP_NUM_THREADS': '1', 'QT_QPA_PLATFORM_PLUGIN_PATH': '/opt/conda/lib/python3.8/site-packages/cv2/qt/plugins', 'QT_QPA_FONTDIR': '/opt/conda/lib/python3.8/site-packages/cv2/qt/fonts', 'NUMEXPR_MAX_THREADS': '7', 'CUBLAS_WORKSPACE_CONFIG': ':4096:8', 'TF_CPP_MIN_LOG_LEVEL': '3', 'TORCH_CPP_LOG_LEVEL': 'ERROR', 'KINETO_LOG_LEVEL': '5'})\u001b[0m\n",
      "\u001b[34m---------------End debug----------------------\u001b[0m\n",
      "\u001b[34mDownloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-seg.pt to 'yolov8m-seg.pt'...\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/52.4M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 10.0M/52.4M [00:00<00:00, 87.6MB/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 20.0M/52.4M [00:00<00:00, 93.6MB/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 30.0M/52.4M [00:00<00:00, 93.9MB/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 40.0M/52.4M [00:00<00:00, 94.1MB/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 50.0M/52.4M [00:00<00:00, 94.3MB/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 52.4M/52.4M [00:00<00:00, 96.7MB/s]\u001b[0m\n",
      "\u001b[34mUltralytics YOLOv8.2.71 🚀 Python-3.8.16 torch-1.12.1+cu113 CPU (Intel Xeon Platinum 8175M 2.50GHz)\u001b[0m\n",
      "\u001b[34mWARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mengine/trainer: #033[0mtask=segment, mode=train, model=yolov8m-seg.pt, data=/opt/ml/input/data/train/data.yaml, epochs=10, time=None, patience=100, batch=5, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/opt/ml/output/data, name=train, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/opt/ml/output/data/train\u001b[0m\n",
      "\u001b[34mDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/755k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 755k/755k [00:00<00:00, 108MB/s]\u001b[0m\n",
      "\u001b[34mOverriding model.yaml nc=80 with nc=9\u001b[0m\n",
      "\u001b[34mfrom  n    params  module                                       arguments\u001b[0m\n",
      "\u001b[34m0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]\u001b[0m\n",
      "\u001b[34m1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]\u001b[0m\n",
      "\u001b[34m2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]\u001b[0m\n",
      "\u001b[34m3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]\u001b[0m\n",
      "\u001b[34m4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]\u001b[0m\n",
      "\u001b[34m5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]\u001b[0m\n",
      "\u001b[34m6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]\u001b[0m\n",
      "\u001b[34m7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]\u001b[0m\n",
      "\u001b[34m8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]\u001b[0m\n",
      "\u001b[34m9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]\u001b[0m\n",
      "\u001b[34m10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\u001b[0m\n",
      "\u001b[34m11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]\u001b[0m\n",
      "\u001b[34m13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\u001b[0m\n",
      "\u001b[34m14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]\u001b[0m\n",
      "\u001b[34m16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]\u001b[0m\n",
      "\u001b[34m17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]\u001b[0m\n",
      "\u001b[34m19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]\u001b[0m\n",
      "\u001b[34m20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]\u001b[0m\n",
      "\u001b[34m22        [15, 18, 21]  1   5164235  ultralytics.nn.modules.head.Segment          [9, 32, 192, [192, 384, 576]]\u001b[0m\n",
      "\u001b[34mYOLOv8m-seg summary: 331 layers, 27,244,859 parameters, 27,244,843 gradients, 110.4 GFLOPs\u001b[0m\n",
      "\u001b[34mTransferred 531/537 items from pretrained weights\u001b[0m\n",
      "\u001b[34mFreezing layer 'model.22.dfl.conv.weight'\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mtrain: #033[0mScanning /opt/ml/input/data/train/train_prueba/labels...:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mtrain: #033[0mScanning /opt/ml/input/data/train/train_prueba/labels... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00<00:00, 598.73it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mtrain: #033[0mNew cache created: /opt/ml/input/data/train/train_prueba/labels.cache\u001b[0m\n",
      "\u001b[34m[2024-08-03 08:48:03.598 algo-1:51 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2024-08-03 08:48:03.993 algo-1:51 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mval: #033[0mScanning /opt/ml/input/data/train/val_prueba/labels...:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mval: #033[0mScanning /opt/ml/input/data/train/val_prueba/labels... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00<00:00, 1154.79it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mval: #033[0mNew cache created: /opt/ml/input/data/train/val_prueba/labels.cache\u001b[0m\n",
      "\u001b[34mPlotting labels to /opt/ml/output/data/train/labels.jpg...\u001b[0m\n",
      "\u001b[34m#033[34m#033[1moptimizer:#033[0m Adam(lr=0.01, momentum=0.937) with parameter groups 86 weight(decay=0.0), 97 weight(decay=0.0005078125), 96 bias(decay=0.0)\u001b[0m\n",
      "\u001b[34mImage sizes 640 train, 640 val\u001b[0m\n",
      "\u001b[34mUsing 0 dataloader workers\u001b[0m\n",
      "\u001b[34mLogging results to #033[1m/opt/ml/output/data/train#033[0m\u001b[0m\n",
      "\u001b[34mStarting training for 10 epochs...\u001b[0m\n",
      "\u001b[34mClosing dataloader mosaic\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m1/10         0G      1.583      3.754      6.472      1.589         10        640:   0%|          | 0/4 [00:07<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1/10         0G      1.583      3.754      6.472      1.589         10        640:  25%|██▌       | 1/4 [00:07<00:21,  7.08s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      1.718          4      5.803      1.747         13        640:  25%|██▌       | 1/4 [00:13<00:21,  7.08s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      1.718          4      5.803      1.747         13        640:  50%|█████     | 2/4 [00:13<00:13,  6.66s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      1.927      5.162      5.427      2.011          8        640:  50%|█████     | 2/4 [00:19<00:13,  6.66s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      1.927      5.162      5.427      2.011          8        640:  75%|███████▌  | 3/4 [00:19<00:06,  6.41s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      2.138      5.456      5.366      2.136         19        640:  75%|███████▌  | 3/4 [00:25<00:06,  6.41s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      2.138      5.456      5.366      2.136         19        640: 100%|██████████| 4/4 [00:25<00:00,  6.33s/it]\u001b[0m\n",
      "\u001b[34m1/10         0G      2.138      5.456      5.366      2.136         19        640: 100%|██████████| 4/4 [00:25<00:00,  6.44s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:03<00:03,  3.75s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mall         20         54    0.00182      0.147    0.00254   0.000916          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m2/10         0G      1.832      4.523      4.951      2.057         11        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2/10         0G      1.832      4.523      4.951      2.057         11        640:  25%|██▌       | 1/4 [00:05<00:17,  5.79s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.189      5.023      4.812      2.294         14        640:  25%|██▌       | 1/4 [00:11<00:17,  5.79s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.189      5.023      4.812      2.294         14        640:  50%|█████     | 2/4 [00:11<00:11,  5.75s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.254      5.203      4.709      2.422          6        640:  50%|█████     | 2/4 [00:17<00:11,  5.75s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.254      5.203      4.709      2.422          6        640:  75%|███████▌  | 3/4 [00:17<00:05,  5.81s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.316      5.179      4.875      2.386         17        640:  75%|███████▌  | 3/4 [00:23<00:05,  5.81s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.316      5.179      4.875      2.386         17        640: 100%|██████████| 4/4 [00:23<00:00,  5.97s/it]\u001b[0m\n",
      "\u001b[34m2/10         0G      2.316      5.179      4.875      2.386         17        640: 100%|██████████| 4/4 [00:23<00:00,  5.90s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:03<00:03,  3.51s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\u001b[0m\n",
      "\u001b[34mall         20         54    0.00403      0.205    0.00551    0.00135          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m3/10         0G      2.551      5.137      5.817      2.752         11        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.551      5.137      5.817      2.752         11        640:  25%|██▌       | 1/4 [00:05<00:17,  5.84s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.534      5.033      5.074      2.758         10        640:  25%|██▌       | 1/4 [00:11<00:17,  5.84s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.534      5.033      5.074      2.758         10        640:  50%|█████     | 2/4 [00:11<00:11,  5.81s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.727      4.972      5.019      2.654         18        640:  50%|█████     | 2/4 [00:17<00:11,  5.81s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.727      4.972      5.019      2.654         18        640:  75%|███████▌  | 3/4 [00:17<00:05,  5.83s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.836       4.99      5.122      2.675         11        640:  75%|███████▌  | 3/4 [00:23<00:05,  5.83s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.836       4.99      5.122      2.675         11        640: 100%|██████████| 4/4 [00:23<00:00,  5.79s/it]\u001b[0m\n",
      "\u001b[34m3/10         0G      2.836       4.99      5.122      2.675         11        640: 100%|██████████| 4/4 [00:23<00:00,  5.80s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:03<00:03,  3.80s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]\u001b[0m\n",
      "\u001b[34mall         20         54      0.129      0.141    0.00472    0.00259          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m4/10         0G      2.798      4.529          5      2.038         13        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m4/10         0G      2.798      4.529          5      2.038         13        640:  25%|██▌       | 1/4 [00:05<00:17,  5.73s/it]\u001b[0m\n",
      "\u001b[34m4/10         0G      2.986      4.538      4.938      2.516         17        640:  25%|██▌       | 1/4 [00:11<00:17,  5.73s/it]\u001b[0m\n",
      "\u001b[34m4/10         0G      2.986      4.538      4.938      2.516         17        640:  50%|█████     | 2/4 [00:11<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[34m4/10         0G      2.894      4.602      4.691      2.692          8        640:  50%|█████     | 2/4 [00:17<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[34m4/10         0G      2.894      4.602      4.691      2.692          8        640:  75%|███████▌  | 3/4 [00:17<00:05,  5.85s/it]\u001b[0m\n",
      "\u001b[34m4/10         0G      3.062      4.763       4.79      2.853         13        640:  75%|███████▌  | 3/4 [00:23<00:05,  5.85s/it]\u001b[0m\n",
      "\u001b[34m4/10         0G      3.062      4.763       4.79      2.853         13        640: 100%|██████████| 4/4 [00:23<00:00,  5.81s/it]#015       4/10         0G      3.062      4.763       4.79      2.853         13        640: 100%|██████████| 4/4 [00:23<00:00,  5.80s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]\u001b[0m\n",
      "\u001b[34mall         20         54      0.444    0.00585    5.2e-05   3.35e-05          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m5/10         0G      3.066      4.843      5.535      2.551         19        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5/10         0G      3.066      4.843      5.535      2.551         19        640:  25%|██▌       | 1/4 [00:05<00:16,  5.63s/it]\u001b[0m\n",
      "\u001b[34m5/10         0G      2.928      4.746      5.084      2.594         13        640:  25%|██▌       | 1/4 [00:11<00:16,  5.63s/it]\u001b[0m\n",
      "\u001b[34m5/10         0G      2.928      4.746      5.084      2.594         13        640:  50%|█████     | 2/4 [00:11<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[34m5/10         0G      3.126       4.77      5.023      2.781         12        640:  50%|█████     | 2/4 [00:17<00:11,  5.71s/it]\u001b[0m\n",
      "\u001b[34m5/10         0G      3.126       4.77      5.023      2.781         12        640:  75%|███████▌  | 3/4 [00:17<00:05,  5.74s/it]\u001b[0m\n",
      "\u001b[34m5/10         0G      3.158       4.92      5.096      2.779          6        640:  75%|███████▌  | 3/4 [00:22<00:05,  5.74s/it]\u001b[0m\n",
      "\u001b[34m5/10         0G      3.158       4.92      5.096      2.779          6        640: 100%|██████████| 4/4 [00:22<00:00,  5.70s/it]#015       5/10         0G      3.158       4.92      5.096      2.779          6        640: 100%|██████████| 4/4 [00:22<00:00,  5.71s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:04<00:04,  4.29s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.38s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]\u001b[0m\n",
      "\u001b[34mall         20         54      0.111     0.0278    3.1e-05    6.2e-06          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m6/10         0G      3.593      4.554      5.069      3.049         13        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m6/10         0G      3.593      4.554      5.069      3.049         13        640:  25%|██▌       | 1/4 [00:05<00:17,  5.72s/it]\u001b[0m\n",
      "\u001b[34m6/10         0G      3.234      4.515       4.63      3.171         12        640:  25%|██▌       | 1/4 [00:11<00:17,  5.72s/it]\u001b[0m\n",
      "\u001b[34m6/10         0G      3.234      4.515       4.63      3.171         12        640:  50%|█████     | 2/4 [00:11<00:11,  5.98s/it]\u001b[0m\n",
      "\u001b[34m6/10         0G      3.303       4.64      4.586      3.283         10        640:  50%|█████     | 2/4 [00:17<00:11,  5.98s/it]\u001b[0m\n",
      "\u001b[34m6/10         0G      3.303       4.64      4.586      3.283         10        640:  75%|███████▌  | 3/4 [00:17<00:05,  5.81s/it]\u001b[0m\n",
      "\u001b[34m6/10         0G       3.13      4.574      4.681      3.125         15        640:  75%|███████▌  | 3/4 [00:23<00:05,  5.81s/it]\u001b[0m\n",
      "\u001b[34m6/10         0G       3.13      4.574      4.681      3.125         15        640: 100%|██████████| 4/4 [00:23<00:00,  5.74s/it]#015       6/10         0G       3.13      4.574      4.681      3.125         15        640: 100%|██████████| 4/4 [00:23<00:00,  5.78s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:04<00:04,  4.45s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]\u001b[0m\n",
      "\u001b[34mall         20         54    0.00055     0.0601     0.0115    0.00387          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m7/10         0G      2.953      4.262      4.855      2.898         11        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m7/10         0G      2.953      4.262      4.855      2.898         11        640:  25%|██▌       | 1/4 [00:05<00:16,  5.49s/it]\u001b[0m\n",
      "\u001b[34m7/10         0G      3.023      4.898      4.591       3.08         11        640:  25%|██▌       | 1/4 [00:11<00:16,  5.49s/it]\u001b[0m\n",
      "\u001b[34m7/10         0G      3.023      4.898      4.591       3.08         11        640:  50%|█████     | 2/4 [00:11<00:11,  5.61s/it]\u001b[0m\n",
      "\u001b[34m7/10         0G      2.949      4.702       4.59      2.984         18        640:  50%|█████     | 2/4 [00:16<00:11,  5.61s/it]\u001b[0m\n",
      "\u001b[34m7/10         0G      2.949      4.702       4.59      2.984         18        640:  75%|███████▌  | 3/4 [00:16<00:05,  5.62s/it]\u001b[0m\n",
      "\u001b[34m7/10         0G      2.836      4.682       4.53      2.924         10        640:  75%|███████▌  | 3/4 [00:22<00:05,  5.62s/it]\u001b[0m\n",
      "\u001b[34m7/10         0G      2.836      4.682       4.53      2.924         10        640: 100%|██████████| 4/4 [00:22<00:00,  5.61s/it]#015       7/10         0G      2.836      4.682       4.53      2.924         10        640: 100%|██████████| 4/4 [00:22<00:00,  5.61s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:05<00:05,  5.21s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.34s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.32s/it]\u001b[0m\n",
      "\u001b[34mall         20         54   0.000274       0.05    0.00212    0.00136          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m8/10         0G       2.59      5.174      4.506      2.604         15        640:   0%|          | 0/4 [00:06<?, ?it/s]\u001b[0m\n",
      "\u001b[34m8/10         0G       2.59      5.174      4.506      2.604         15        640:  25%|██▌       | 1/4 [00:06<00:18,  6.05s/it]\u001b[0m\n",
      "\u001b[34m8/10         0G      2.607      5.031      4.368      2.698         10        640:  25%|██▌       | 1/4 [00:11<00:18,  6.05s/it]\u001b[0m\n",
      "\u001b[34m8/10         0G      2.607      5.031      4.368      2.698         10        640:  50%|█████     | 2/4 [00:11<00:11,  5.89s/it]\u001b[0m\n",
      "\u001b[34m8/10         0G       2.73      4.927      4.496      2.795         11        640:  50%|█████     | 2/4 [00:17<00:11,  5.89s/it]\u001b[0m\n",
      "\u001b[34m8/10         0G       2.73      4.927      4.496      2.795         11        640:  75%|███████▌  | 3/4 [00:17<00:05,  5.75s/it]\u001b[0m\n",
      "\u001b[34m8/10         0G      2.944      4.821      4.478      2.924         15        640:  75%|███████▌  | 3/4 [00:23<00:05,  5.75s/it]\u001b[0m\n",
      "\u001b[34m8/10         0G      2.944      4.821      4.478      2.924         15        640: 100%|██████████| 4/4 [00:23<00:00,  5.72s/it]#015       8/10         0G      2.944      4.821      4.478      2.924         15        640: 100%|██████████| 4/4 [00:23<00:00,  5.77s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWARNING ⚠️ NMS time limit 2.500s exceeded\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:05<00:05,  5.94s/it]\u001b[0m\n",
      "\u001b[34mWARNING ⚠️ NMS time limit 2.500s exceeded\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.98s/it]\u001b[0m\n",
      "\u001b[34mall         20         54   0.000419     0.0278   0.000261    7.8e-05          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m9/10         0G      2.504      4.518       4.16      2.513         19        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m9/10         0G      2.504      4.518       4.16      2.513         19        640:  25%|██▌       | 1/4 [00:05<00:16,  5.54s/it]\u001b[0m\n",
      "\u001b[34m9/10         0G      2.682      4.702       4.26      2.814         10        640:  25%|██▌       | 1/4 [00:11<00:16,  5.54s/it]\u001b[0m\n",
      "\u001b[34m9/10         0G      2.682      4.702       4.26      2.814         10        640:  50%|█████     | 2/4 [00:11<00:11,  5.57s/it]\u001b[0m\n",
      "\u001b[34m9/10         0G       2.76      4.665      4.207      2.953          9        640:  50%|█████     | 2/4 [00:16<00:11,  5.57s/it]\u001b[0m\n",
      "\u001b[34m9/10         0G       2.76      4.665      4.207      2.953          9        640:  75%|███████▌  | 3/4 [00:16<00:05,  5.68s/it]\u001b[0m\n",
      "\u001b[34m9/10         0G          3       4.62      4.292      3.092         13        640:  75%|███████▌  | 3/4 [00:22<00:05,  5.68s/it]\u001b[0m\n",
      "\u001b[34m9/10         0G          3       4.62      4.292      3.092         13        640: 100%|██████████| 4/4 [00:22<00:00,  5.66s/it]#015       9/10         0G          3       4.62      4.292      3.092         13        640: 100%|██████████| 4/4 [00:22<00:00,  5.64s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:04<00:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.32s/it]\u001b[0m\n",
      "\u001b[34mall         20         54   0.000419     0.0278   0.000261    7.8e-05          0          0          0          0\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\u001b[0m\n",
      "\u001b[34m10/10         0G      2.843      4.541      4.531      3.095         17        640:   0%|          | 0/4 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m10/10         0G      2.843      4.541      4.531      3.095         17        640:  25%|██▌       | 1/4 [00:05<00:16,  5.61s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      3.185      4.676       4.66      3.133         14        640:  25%|██▌       | 1/4 [00:11<00:16,  5.61s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      3.185      4.676       4.66      3.133         14        640:  50%|█████     | 2/4 [00:11<00:11,  5.65s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      3.107      4.814      4.541      3.125          9        640:  50%|█████     | 2/4 [00:16<00:11,  5.65s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      3.107      4.814      4.541      3.125          9        640:  75%|███████▌  | 3/4 [00:16<00:05,  5.62s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      2.877      4.622      4.396       2.98         10        640:  75%|███████▌  | 3/4 [00:22<00:05,  5.62s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      2.877      4.622      4.396       2.98         10        640: 100%|██████████| 4/4 [00:22<00:00,  5.61s/it]\u001b[0m\n",
      "\u001b[34m10/10         0G      2.877      4.622      4.396       2.98         10        640: 100%|██████████| 4/4 [00:22<00:00,  5.62s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:04<00:04,  4.17s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]\u001b[0m\n",
      "\u001b[34mall         20         54   0.000419     0.0278   0.000261    7.8e-05          0          0          0          0\u001b[0m\n",
      "\u001b[34m10 epochs completed in 0.093 hours.\u001b[0m\n",
      "\u001b[34mOptimizer stripped from /opt/ml/output/data/train/weights/last.pt, 54.8MB\u001b[0m\n",
      "\u001b[34mOptimizer stripped from /opt/ml/output/data/train/weights/best.pt, 54.8MB\u001b[0m\n",
      "\u001b[34mValidating /opt/ml/output/data/train/weights/best.pt...\u001b[0m\n",
      "\u001b[34mUltralytics YOLOv8.2.71 🚀 Python-3.8.16 torch-1.12.1+cu113 CPU (Intel Xeon Platinum 8175M 2.50GHz)\u001b[0m\n",
      "\u001b[34mYOLOv8m-seg summary (fused): 245 layers, 27,227,595 parameters, 0 gradients, 110.0 GFLOPs\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  50%|█████     | 1/2 [00:04<00:04,  4.47s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.87s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.81s/it]\u001b[0m\n",
      "\u001b[34mall         20         54   0.000548     0.0601    0.00399    0.00155          0          0          0          0\u001b[0m\n",
      "\u001b[34mmis_lost          7         11   0.000969     0.0909    0.00163   0.000664          0          0          0          0\n",
      "              met_tear          3          6          0          0          0          0          0          0          0          0\n",
      "           met_scratch          6         19          0          0          0          0          0          0          0          0\u001b[0m\n",
      "\u001b[34mglass_crack          4          4    0.00054       0.25    0.00043   0.000169          0          0          0          0\u001b[0m\n",
      "\u001b[34mmis_punct          1          1          0          0          0          0          0          0          0          0\u001b[0m\n",
      "\u001b[34mmis_lamp          1          1          0          0          0          0          0          0          0          0\u001b[0m\n",
      "\u001b[34mmet_dent_minor          4          5          0          0          0          0          0          0          0          0\u001b[0m\n",
      "\u001b[34mmet_dent_medium          2          2          0          0          0          0          0          0          0          0\u001b[0m\n",
      "\u001b[34mmet_dent_severe          3          5    0.00342        0.2     0.0338     0.0131          0          0          0          0\u001b[0m\n",
      "\u001b[34mSpeed: 0.6ms preprocess, 289.8ms inference, 0.0ms loss, 116.0ms postprocess per image\u001b[0m\n",
      "\u001b[34mResults saved to #033[1m/opt/ml/output/data/train#033[0m\u001b[0m\n",
      "\u001b[34mUltralytics YOLOv8.2.71 🚀 Python-3.8.16 torch-1.12.1+cu113 CPU (Intel Xeon Platinum 8175M 2.50GHz)\u001b[0m\n",
      "\u001b[34mYOLOv8m-seg summary (fused): 245 layers, 27,227,595 parameters, 0 gradients, 110.0 GFLOPs\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mPyTorch:#033[0m starting from '/opt/ml/output/data/train/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 45, 8400), (1, 32, 160, 160)) (52.3 MB)\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mTorchScript:#033[0m starting export with torch 1.12.1+cu113...\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mTorchScript:#033[0m export success ✅ 3.5s, saved as '/opt/ml/output/data/train/weights/best.torchscript' (104.4 MB)\u001b[0m\n",
      "\u001b[34mExport complete (6.1s)\u001b[0m\n",
      "\u001b[34mResults saved to #033[1m/opt/ml/output/data/train/weights#033[0m\u001b[0m\n",
      "\u001b[34mPredict:         yolo predict task=segment model=/opt/ml/output/data/train/weights/best.torchscript imgsz=640  \u001b[0m\n",
      "\u001b[34mValidate:        yolo val task=segment model=/opt/ml/output/data/train/weights/best.torchscript imgsz=640 data=/opt/ml/input/data/train/data.yaml  \u001b[0m\n",
      "\u001b[34mVisualize:       https://netron.app\u001b[0m\n",
      "\u001b[34m2024-08-03 08:54:09,697 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-08-03 08:54:09,697 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-08-03 08:54:09,697 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-08-03 08:54:27 Uploading - Uploading generated training model\n",
      "2024-08-03 08:54:27 Completed - Training job completed\n",
      "Training seconds: 1020\n",
      "Billable seconds: 435\n",
      "Managed Spot Training savings: 57.4%\n"
     ]
    }
   ],
   "source": [
    "train_data_path = f's3://{bucket_name}'\n",
    "output_path = f's3://{bucket_name}/output'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role_arn,\n",
    "    image_uri=image_uri, \n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    # instance_type='ml.m5.12xlarge',\n",
    "    framework_version=\"1.12.1\",\n",
    "    py_version=\"py38\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        'epochs': 10,\n",
    "        'optimizer': 'Adam',\n",
    "        'lr0': 0.01,\n",
    "        'lrf': 0.01,\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 0.0005\n",
    "    },\n",
    "    use_spot_instances=True,\n",
    "    # input_mode='File',  # FastFile causes a issue with writing label cache\n",
    "    debugger_hook_config=False,\n",
    "    max_wait=360000+3600,\n",
    "    max_run=360000,\n",
    "    output_path=output_path,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "estimator.fit({\"train\": train_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color=\"#62b6cb\"> 2.3 Hyperparameter tuning in Sagemaker  <a name=\"id23\"></a> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#62b6cb\"> 2.3.1 Hyperparameters space and tuning code for Sagemaker  <a name=\"id231\"></a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework='pytorch',\n",
    "    region='eu-west-1',\n",
    "    version='1.12.1',\n",
    "    py_version='py38',\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    # instance_type='ml.m5.12xlarge',\n",
    "    # instance_type='ml.g4dn.xlarge',\n",
    "    image_scope='training'\n",
    ")\n",
    "\n",
    "\n",
    "metric_definitions = [\n",
    "    {\n",
    "        \"Name\": \"box_loss\", \n",
    "        \"Regex\": \"Epoch\\\\s+\\\\d+\\\\/\\\\d+\\\\s+\\\\d+G\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"cls_loss\", \n",
    "        \"Regex\": \"Epoch\\\\s+\\\\d+\\\\/\\\\d+\\\\s+\\\\d+G\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"dfl_loss\", \n",
    "        \"Regex\": \"Epoch\\\\s+\\\\d+\\\\/\\\\d+\\\\s+\\\\d+G\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+(\\\\d+\\\\.\\\\d+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"mAP50\", \n",
    "        \"Regex\": \"all\\\\s+\\\\d+\\\\s+\\\\d+\\\\s+\\\\d+\\\\.\\\\d+\\\\s+\\\\d+\\\\.\\\\d+\\\\s+(\\\\d+\\\\.\\\\d+)\\\\s+\\\\d+\\\\.\\\\d+\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"mAP50-95\", \n",
    "        \"Regex\": \"all\\\\s+\\\\d+\\\\s+\\\\d+\\\\s+\\\\d+\\\\.\\\\d+\\\\s+\\\\d+\\\\.\\\\d+\\\\s+\\\\d+\\\\.\\\\d+\\\\s+(\\\\d+\\\\.\\\\d+)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'batch': CategoricalParameter([32, 64, 128]),\n",
    "    'lr0': CategoricalParameter([1e-5, 1e-4, 1e-3, 1e-2, 1e-1]),\n",
    "    'lrf': CategoricalParameter([0.01, 0.1, 0.5, 1]),\n",
    "    'momentum': CategoricalParameter([0.6, 0.7, 0.8, 0.9, 0.98]),\n",
    "    'weight_decay': CategoricalParameter([0.0001, 0.001])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameter_ranges = {\n",
    "#     'epochs': IntegerParameter(100, 500, scaling_type='Auto'),\n",
    "#     'batch': IntegerParameter(32, 68, scaling_type='Auto'),\n",
    "#     'lr0': ContinuousParameter(1e-5, 1e-1, scaling_type='Auto'),\n",
    "#     'lrf': ContinuousParameter(0.01, 1, scaling_type='Auto'),\n",
    "#     'momentum': ContinuousParameter(0.6, 0.98, scaling_type='Auto'),\n",
    "#     'weight_decay': ContinuousParameter(0.0, 0.001, scaling_type='Auto')\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos_lr = True (Utilizes a cosine learning rate scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.)\n",
    "patience = 30\n",
    "warmup_epochs = 10 ()\n",
    "hyperparameter_ranges = {\n",
    "    'epochs': IntegerParameter(100, 500, scaling_type='Auto'),\n",
    "    'batch': CategoricalParameter([32, 64, 128, 256, 512]),\n",
    "    'lr0': ContinuousParameter(1e-5, 1e-1, scaling_type='Auto'),\n",
    "    'lrf': ContinuousParameter(0.01, 1, scaling_type='Auto'),\n",
    "    'momentum': ContinuousParameter(0.6, 0.98, scaling_type='Auto'),\n",
    "    'weight_decay': ContinuousParameter(0.0, 0.01, scaling_type='Auto')\n",
    "}\n",
    "\n",
    "ray -> warmup  tune.uniform(0.0, 5.0)\n",
    "weigth_decay tune.uniform(0.0, 0.001)\t\n",
    "dropout ->  'dropout': ContinuousParameter(0.0, 0.5, scaling_type='Auto') (chatgpt)\n",
    "\n",
    "\n",
    "\n",
    " https://github.com/aws/amazon-sagemaker-examples/blob/main/hyperparameter_tuning/pytorch_mnist/hpo_pytorch_mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_tune.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install ultralytics\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters\n",
    "    \n",
    "    parser.add_argument('--epochs',type=int, help='number of training epochs')\n",
    "    parser.add_argument('--patience',type=int, help='patience value')\n",
    "    parser.add_argument(\"--batch\", type=int, help='batch size')\n",
    "    \n",
    "    parser.add_argument('--optimizer', type=str, help='optimizer to use')\n",
    "    parser.add_argument('--lr0', type=float, help='initial learning rate')\n",
    "    parser.add_argument('--lrf', type=float, help='final learning rate')\n",
    "    parser.add_argument('--momentum', type=float, help='momentum')\n",
    "    parser.add_argument('--weight_decay', type=float, help='optimizer weight decay')\n",
    "\n",
    "    # SageMaker specific arguments\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--runs-path\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "\n",
    "   \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('---------------Debug injected environment and arguments--------------------')\n",
    "    print(sys.argv)\n",
    "    print(os.environ)\n",
    "    print('---------------End debug----------------------')\n",
    "\n",
    "  \n",
    "    # Train the YOLO model\n",
    "    # yolo_model = YOLO(os.path.join(args.weights_yolo_path, \"yolov8m-seg.pt\"))\n",
    "    yolo_model = YOLO(\"yolov8m-seg.pt\")\n",
    "    yolo_model.train(data=os.path.join(args.train, \"data.yaml\"), \n",
    "                     batch=args.batch,\n",
    "                     epochs=args.epochs, \n",
    "                     optimizer=args.optimizer, \n",
    "                     lr0=args.lr0, \n",
    "                     lrf=args.lrf, \n",
    "                     momentum=args.momentum,\n",
    "                     weight_decay=args.weight_decay,\n",
    "                     patience=args.patience,\n",
    "                     task='segment',\n",
    "                     project=args.runs_path)\n",
    "    \n",
    "    yolo_model.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding hyperparameters we are going to do the following:\n",
    "\n",
    "+ epochs: We will not tune this parameter as we are already including hyperparameters to tackle the overfitting. We will assume a big enough number so that the model does not underfit. Besides YOLO model selects the weights of the best performance epoch.\n",
    "+ batch size: For the tuning we just try powers of 2 as it is recommended for parallel calculation efficiency. to reduce the computing time we will try medium / high batch sizes (minimum of 32), but for this we will need to use GPU instance from Sagemaker as are more appropiate for deep learning model training\n",
    "+ Learning Rate: As we will be using a random seach instead of a hyperband method we will be using a list of given parameters as suggested in this [article](https://www.sciencedirect.com/science/article/pii/S2666827022000433).\n",
    "+ Learning rate factor (decay the learning rate over time): Again as we will be using a random seach instead of a hyperband method we will be using a list of given parameters as it would lead to faster convergence and lower computational costs.\n",
    "+ momentum: It control the exponential decay rates for the moment estimates. As mentioned in the Ultralytics [webpage](https://docs.ultralytics.com/integrations/ray-tune/#default-search-space-description) we are using the tipical range of values to tune.\n",
    "+ Weight Decay: To reduce overfitting we will introduce a weight decay always but very small.\n",
    "+ Optimization: we are using adam optimizacion as it is quite popular for deep learning models and it has also been recommended in this [article](https://www.sciencedirect.com/science/article/pii/S2666827022000433).\n",
    "+ Early stopping: Of 30 epochs so if it is already not improving we earn some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator\n",
    "train_data_path = f's3://{bucket_name}'\n",
    "output_path = f's3://{bucket_name}/output'\n",
    "\n",
    "estimator_tune = PyTorch(\n",
    "    entry_point=\"train_tune.py\",\n",
    "    role=role_arn,\n",
    "    image_uri=image_uri, \n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    # instance_type='ml.m5.12xlarge',\n",
    "     # instance_type='ml.p3.2xlarge', #GPU best for our problem\n",
    "    framework_version=\"1.12.1\",\n",
    "    py_version=\"py38\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        'optimizer': 'Adam',\n",
    "        'epochs': 100,\n",
    "        'patience': 30\n",
    "    },\n",
    "    use_spot_instances=True,\n",
    "    # input_mode='File',  # FastFile causes a issue with writing label cache\n",
    "    debugger_hook_config=False,\n",
    "    max_wait=360000+3600,\n",
    "    max_run=360000,\n",
    "    output_path=output_path,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(estimator_tune, \n",
    "                            objective_metric_name=\"mAP50-95\", \n",
    "                            metric_definitions=metric_definitions, \n",
    "                            hyperparameter_ranges= hyperparameter_ranges, \n",
    "                            #strategy='Hyperband',\n",
    "                            strategy='Random', \n",
    "                            max_jobs=2,  # Only one job needed since hyperparameters are constant\n",
    "                            max_parallel_jobs=2  # Only one job needed since hyperparameters are constant\n",
    "                            # strategy_config = StrategyConfig(hyperband_strategy_config=HyperbandStrategyConfig(max_resource=10, min_resource = 1))\n",
    "                           )\n",
    "tuner.fit({\"train\": train_data_path}, job_name=\"YOLO-tuning2\")\n",
    "\n",
    "#  ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,31}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"#62b6cb\"> 2.2.2 Select best tuning model and metrics  <a name=\"id222\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the models trained by the tuner and the metrics of the model. The YOLO model selects the best weights through the epochs by defining the metric `best_metric = mAP50 * 0.9 + mAP50-95 * 0.1`. For more information on how this metric is defined and used to choose the best weights, please check this [link to the explanation](https://github.com/ultralytics/ultralytics/issues/14873).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tuning = \"pytorch-training-240803-1623\"\n",
    "\n",
    "\n",
    "# stablish conexion with s3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Retrieve training job names from the dataframe\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(name_tuning)\n",
    "results_df = tuner_analytics.dataframe()\n",
    "training_job_names = results_df[\"TrainingJobName\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>epochs</th>\n",
       "      <th>lr0</th>\n",
       "      <th>lrf</th>\n",
       "      <th>momentum</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>0.673789</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>pytorch-training-240803-1623-004-af4c48cf</td>\n",
       "      <td>Completed</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-03 18:34:37+01:00</td>\n",
       "      <td>2024-08-03 19:12:48+01:00</td>\n",
       "      <td>2291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.063731</td>\n",
       "      <td>0.837243</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>pytorch-training-240803-1623-003-376c31a1</td>\n",
       "      <td>Completed</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-03 18:09:56+01:00</td>\n",
       "      <td>2024-08-03 18:31:57+01:00</td>\n",
       "      <td>1321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.619900</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>pytorch-training-240803-1623-002-ebe489f6</td>\n",
       "      <td>Completed</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-03 17:49:12+01:00</td>\n",
       "      <td>2024-08-03 18:07:42+01:00</td>\n",
       "      <td>1110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.315215</td>\n",
       "      <td>0.821607</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>pytorch-training-240803-1623-001-e94cea16</td>\n",
       "      <td>Completed</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-03 17:24:30+01:00</td>\n",
       "      <td>2024-08-03 17:47:45+01:00</td>\n",
       "      <td>1395.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch  epochs       lr0       lrf  momentum  weight_decay  \\\n",
       "0   46.0    18.0  0.000074  0.015310  0.673789      0.000995   \n",
       "1   66.0    22.0  0.000070  0.063731  0.837243      0.000775   \n",
       "2   38.0    16.0  0.000142  0.018377  0.619900      0.000789   \n",
       "3   32.0    24.0  0.000731  0.315215  0.821607      0.000080   \n",
       "\n",
       "                             TrainingJobName TrainingJobStatus  \\\n",
       "0  pytorch-training-240803-1623-004-af4c48cf         Completed   \n",
       "1  pytorch-training-240803-1623-003-376c31a1         Completed   \n",
       "2  pytorch-training-240803-1623-002-ebe489f6         Completed   \n",
       "3  pytorch-training-240803-1623-001-e94cea16         Completed   \n",
       "\n",
       "  FinalObjectiveValue         TrainingStartTime           TrainingEndTime  \\\n",
       "0                None 2024-08-03 18:34:37+01:00 2024-08-03 19:12:48+01:00   \n",
       "1                None 2024-08-03 18:09:56+01:00 2024-08-03 18:31:57+01:00   \n",
       "2                None 2024-08-03 17:49:12+01:00 2024-08-03 18:07:42+01:00   \n",
       "3                None 2024-08-03 17:24:30+01:00 2024-08-03 17:47:45+01:00   \n",
       "\n",
       "   TrainingElapsedTimeSeconds  \n",
       "0                      2291.0  \n",
       "1                      1321.0  \n",
       "2                      1110.0  \n",
       "3                      1395.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch-training-240803-1623-001-e94cea16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>batch</th>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr0</th>\n",
       "      <td>0.000731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lrf</th>\n",
       "      <td>0.315215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>momentum</th>\n",
       "      <td>0.821607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decay</th>\n",
       "      <td>0.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <td>Completed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <td>0.042237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <td>2024-08-03 16:24:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <td>2024-08-03 16:47:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <td>1395.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           pytorch-training-240803-1623-001-e94cea16\n",
       "batch                                                           32.0\n",
       "epochs                                                          24.0\n",
       "lr0                                                         0.000731\n",
       "lrf                                                         0.315215\n",
       "momentum                                                    0.821607\n",
       "weight_decay                                                 0.00008\n",
       "TrainingJobStatus                                          Completed\n",
       "FinalObjectiveValue                                         0.042237\n",
       "TrainingStartTime                          2024-08-03 16:24:30+00:00\n",
       "TrainingEndTime                            2024-08-03 16:47:45+00:00\n",
       "TrainingElapsedTimeSeconds                                    1395.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tuning = \"pytorch-training-240803-1623\"\n",
    "\n",
    "# stablish conexion with s3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Retrieve training job names from the dataframe\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(name_tuning)\n",
    "results_df = tuner_analytics.dataframe()\n",
    "training_job_names = results_df[\"TrainingJobName\"].tolist()\n",
    "\n",
    "# Create a dictionary to store the paths of the .tar.gz files\n",
    "output_targz_dict = {\n",
    "    job_name: file['Key']\n",
    "    for file in s3.list_objects_v2(Bucket=bucket_name2).get(\"Contents\", [])\n",
    "    for job_name in training_job_names\n",
    "    if job_name in file['Key']\n",
    "}\n",
    "\n",
    "\n",
    "results_df.set_index('TrainingJobName', inplace=True)\n",
    "\n",
    "# Process each .tar.gz file and update the dataframe\n",
    "for job_name, tar_gz_file in output_targz_dict.items():\n",
    "    response = s3.get_object(Bucket=bucket_name2, Key=tar_gz_file)\n",
    "    tar_gz_data = response['Body'].read()\n",
    "\n",
    "    with tarfile.open(fileobj=BytesIO(tar_gz_data), mode='r:gz') as tar:\n",
    "        # Look for the 'train/results.csv' file inside the .tar.gz\n",
    "        csv_file = tar.extractfile('train/results.csv')\n",
    "        if csv_file:\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Calculate the combined metric\n",
    "            df['combined_metric'] = df['metrics/mAP50(M)'] * 0.9 + df['metrics/mAP50-95(M)'] * 0.1\n",
    "            results_df.loc[job_name, \"FinalObjectiveValue\"] = df['combined_metric'].max()\n",
    "            \n",
    "        else:\n",
    "            print(f\"'train/results.csv' not found in {tar_gz_file}\")\n",
    "\n",
    "\n",
    "# FinalObjectiveValue = mAP50 * 0.9 + mAP50-95 * 0.1 (Segmentation)\n",
    "results_df = results_df.sort_values(by='FinalObjectiveValue', ascending=False)\n",
    "pd.DataFrame(results_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the best model is detected, the output of this model is download it in the Models in the folder best_model_tuning_{name of the tuning joob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been extracted to ./Models/best_modeltuning_pytorch-training-240803-1623\n"
     ]
    }
   ],
   "source": [
    "# obtain output of the best job \n",
    "best_jobname = results_df.index[0]\n",
    "targz_bestmodel = output_targz_dict[best_jobname]\n",
    "path_bestmodel = os.path.join(path_models, f'best_modeltuning_{name_tuning}')\n",
    "\n",
    "if os.path.exists(path_bestmodel):\n",
    "    shutil.rmtree(path_bestmodel)\n",
    "    os.makedirs(path_bestmodel)\n",
    "else:\n",
    "    os.makedirs(path_bestmodel)\n",
    "    \n",
    "# Download the .tar.gz file from S3\n",
    "response = s3.get_object(Bucket=bucket_name2, Key=targz_bestmodel)\n",
    "tar_gz_data = response['Body'].read()\n",
    "with tarfile.open(fileobj=BytesIO(tar_gz_data), mode='r:gz') as tar:\n",
    "    tar.extractall(path=path_bestmodel)\n",
    "\n",
    "print(f\"The model has been extracted to {path_bestmodel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation metrics need to be obtained again as best.pt does not have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation: 20 \n",
      "\n",
      "\n",
      "Ultralytics YOLOv8.2.74 🚀 Python-3.10.14 torch-2.0.0.post104 CPU (Intel Xeon Platinum 8488C)\n",
      "YOLOv8m-seg summary (fused): 245 layers, 27,227,595 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/sagemaker-user/project-danielteresa/Data/Yoloimages/train_prueba/labels... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00<00:00, 916.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/sagemaker-user/project-danielteresa/Data/Yoloimages/train_prueba/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         51      0.724     0.0732      0.314      0.208      0.737     0.0667      0.118      0.053\n",
      "              mis_lost          3          3      0.377      0.333      0.512      0.427      0.759      0.333      0.512      0.274\n",
      "              met_tear          7          8          1          0      0.155      0.132          1          0          0          0\n",
      "           met_scratch          9         19      0.344     0.0526      0.125     0.0493          0          0    0.00777    0.00185\n",
      "           glass_crack          4          5          1          0       0.34      0.281          1          0      0.229     0.0668\n",
      "             mis_punct          5          7          1          0       0.17      0.116          1          0          0          0\n",
      "        met_dent_minor          1          1          1          0      0.995      0.497          1          0          0          0\n",
      "       met_dent_medium          1          3          1          0          0          0          1          0          0          0\n",
      "       met_dent_severe          5          5     0.0685        0.2      0.213      0.164      0.138        0.2      0.196     0.0813\n",
      "Speed: 0.6ms preprocess, 169.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/sagemaker-user/project-danielteresa/Models/best_modeltuning_pytorch-training-240803-1623/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics\n",
    "\n",
    "# try with train\n",
    "path_bestmodel = os.path.join(path_models, f'best_modeltuning_pytorch-training-240803-1623')\n",
    "path_s3_val ='val_prueba'\n",
    "\n",
    "# charge the best tuning mmodel\n",
    "tuning_model = YOLO(os.path.join(path_bestmodel, 'train/weights/best.pt'))\n",
    "# path_s3_val ='val_prueba'\n",
    "path_yolo_val = os.path.join(path_yolo, path_s3_val) #creates the val folder\n",
    "\n",
    "if os.path.exists(path_yolo_val):\n",
    "    shutil.rmtree(path_yolo_val)\n",
    "\n",
    "\n",
    "# Download the val folder from S3\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = s3_resource.Bucket(bucket_name2)\n",
    "\n",
    "for obj in bucket.objects.filter(Prefix = path_s3_val): \n",
    "        # check that the images and labels folder exits\n",
    "        if not os.path.exists(os.path.join(path_yolo, os.path.dirname(obj.key))):\n",
    "            os.makedirs(os.path.join(path_yolo , os.path.dirname(obj.key)))\n",
    "        # save content of val folder of S3\n",
    "        target = os.path.join(path_yolo, obj.key)\n",
    "        bucket.download_file(obj.key, target) \n",
    "        \n",
    "print(f\"Number of images in validation: {len(os.listdir(f'{path_yolo_val}/images'))} \\n\\n\")\n",
    "\n",
    "\n",
    "# create a yaml for the tuning model as yolo needs the absolute dir. and sometimes we are working in different computers \n",
    "\n",
    "# obtain the val classes\n",
    "with open(os.path.join(path_annotations,val_annotations_name), 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "names = [class_name['name'] for class_name in coco_data[\"categories\"]]\n",
    "\n",
    "# Specify the paths and information\n",
    "nc = len(names)\n",
    "path_yaml_tune = './Notebooks/data_tune.yaml'\n",
    "path_yolo_val_abs =  os.path.join(os.getcwd(), f'{path_yolo_val}/images')\n",
    "create_yaml_file(path_yaml_tune, None,  path_yolo_val_abs , nc, names)\n",
    "\n",
    "# calculate the results of the metrics for val (the output will be store path_bestmodel,\\val)\n",
    "if os.path.exists(os.path.join(path_bestmodel, 'val')):\n",
    "    shutil.rmtree(os.path.join(path_bestmodel, 'val'))\n",
    "\n",
    "metrics = tuning_model.val(data = path_yaml_tune, project =  os.path.join(os.getcwd(), os.path.join(path_bestmodel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Models/best_modeltuning_pytorch-training-240803-1623'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuning_model.metrics.save_dir\n",
    "path_bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmetrics_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_results_yolo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "This function provides a comprehensive analysis of YOLO model performance by generating a styled table of key metrics,\n",
       "visualizing the confusion matrix, and loading the results of the training/validation process from  CSV results file generated by \n",
       "yolo.\n",
       "\n",
       "Parameters:\n",
       "- metrics: A metrics object from the YOLO model's validation or testing process, containing various performance metrics.\n",
       "- path_results_yolo: The file path to the directory where YOLO training/validation results are stored, including the 'results.csv' file.\n",
       "- color1: A string representing the color (in hexadecimal or named color format) to be used for certain visualizations, particularly in the styled table and confusion matrix.\n",
       "- color2: A string representing an additional color to be used for visualizations if needed.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/project-danielteresa/src/mymodule.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_yolo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for painting the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmetrics_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtuning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_results_yolo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_bestmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolor2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#In case of wanting delete the val folder\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# if os.path.exists(path_yolo_val):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     shutil.rmtree(path_yolo_val)\u001b[39;00m\n",
      "File \u001b[0;32m~/project-danielteresa/src/mymodule.py:1600\u001b[0m, in \u001b[0;36mmetrics_yolo\u001b[0;34m(model, path_results_yolo, color1, color2)\u001b[0m\n\u001b[1;32m   1598\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m   1599\u001b[0m names \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m-> 1600\u001b[0m \u001b[43mprint_styled_metrics_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m confusion_matrix_yolo(metrics, names, color1)\n\u001b[1;32m   1603\u001b[0m df_epochs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_results_yolo,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/project-danielteresa/src/mymodule.py:1449\u001b[0m, in \u001b[0;36mprint_styled_metrics_table\u001b[0;34m(metrics, names, color)\u001b[0m\n\u001b[1;32m   1446\u001b[0m seg_f1 \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mseg\u001b[38;5;241m.\u001b[39mf1\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m-> 1449\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClass Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBox Precision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBox Recall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBox F1 Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_f1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBox AP50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_ap50\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBox AP50-95\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_ap5095\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation Precision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation Recall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation F1 Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_f1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation AP50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_ap50\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation AP50-95\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_ap5095\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1463\u001b[0m all_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox Precision\u001b[39m\u001b[38;5;124m\"\u001b[39m: [metrics\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmp],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentation AP50-95\u001b[39m\u001b[38;5;124m\"\u001b[39m: [metrics\u001b[38;5;241m.\u001b[39mseg\u001b[38;5;241m.\u001b[39mmap]\n\u001b[1;32m   1475\u001b[0m })\n\u001b[1;32m   1477\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, all_results], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# metrics_yolo(model = tuning_model, path_results_yolo = os.path.join(path_bestmodel, 'train'), color1 = color1, color2 = color2)\n",
    "metrics_yolo(model = tuning_model, path_results_yolo = os.path.join(path_bestmodel, 'val'), color1 = color1, color2 = color2)\n",
    "#In case of wanting delete the val folder\n",
    "\n",
    "# if os.path.exists(path_yolo_val):\n",
    "#     shutil.rmtree(path_yolo_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 3. Train with the final model   <a name=\"id3\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 3.1. Train and Validation Datasets Union  <a name=\"id31\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a directory of *train_final* including the validation and the train images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Data/Yoloimages/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and labels have been successfully downloaded to ./Data/Yoloimages/train_final\n"
     ]
    }
   ],
   "source": [
    "# path_s3_val ='val_prueba'\n",
    "path_yolo_train_final = os.path.join(path_yolo, 'train_final') #creates the train final folder\n",
    "\n",
    "if os.path.exists(path_yolo_train_final):\n",
    "    shutil.rmtree(path_yolo_train_final)\n",
    "\n",
    "# Create the images and labels folders locally\n",
    "images_final_path = os.path.join(path_yolo_train_final, 'images')\n",
    "labels_final_path = os.path.join(path_yolo_train_final, 'labels')\n",
    "\n",
    "os.makedirs(images_final_path, exist_ok=True)\n",
    "os.makedirs(labels_final_path, exist_ok=True)\n",
    "\n",
    "# Download the val folder from S3\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = s3_resource.Bucket(bucket_name2)\n",
    "\n",
    "# Define S3 folders\n",
    "folders = ['train_prueba/', 'val_prueba/']\n",
    "\n",
    "# Download images and labels from each S3 folder to the local directory\n",
    "for folder in folders:\n",
    "    # Download images\n",
    "    path_s3_images = os.path.join(folder, 'images/')\n",
    "    for obj in bucket.objects.filter(Prefix=path_s3_images):\n",
    "        if obj.key.endswith('/'):\n",
    "            continue  # Skip directories\n",
    "        target = os.path.join(images_final_path, os.path.basename(obj.key))\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "    # Download labels\n",
    "    path_s3_labels = os.path.join(folder, 'labels/')\n",
    "    for obj in bucket.objects.filter(Prefix=path_s3_labels):\n",
    "        if obj.key.endswith('/'):\n",
    "            continue  # Skip directories\n",
    "        target = os.path.join(labels_final_path, os.path.basename(obj.key))\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "print(\"Images and labels have been successfully downloaded to\", path_yolo_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Data/Yoloimages/train_final: 100%|██████████| 400/400 [00:02<00:00, 133.40file/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the bucket name and the folder to copy to S3\n",
    "bucket_train_final = \"sagemaker-eu-west-1-project-danielteresa/train_final\"\n",
    "local_train_final = \"Data/Yoloimages/train_final\"\n",
    "\n",
    "# Upload train final\n",
    "upload_folder_to_s3(local_train_final, bucket_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 3.2. Retrain the Final Model in Sagemaker  <a name=\"id32\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the hyperparameters of the best modek in validation and we create a diccionary with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first row as a dictionary\n",
    "selected_params = results_df.iloc[0].to_dict()\n",
    "\n",
    "# Remove non-hyperparameter keys\n",
    "selected_params = {k: v for k, v in selected_params.items() if k in ['batch', 'lr0', 'lrf', 'momentum', 'weight_decay']}\n",
    "\n",
    "# Convert to appropriate types\n",
    "selected_params = {k: int(v) if k == 'batch' else float(v) for k, v in selected_params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_train_final = PyTorch(\n",
    "    entry_point=\"train_tune.py\",\n",
    "    role=role_arn,\n",
    "    image_uri=image_uri, \n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    # instance_type='ml.m5.12xlarge',\n",
    "     # instance_type='ml.p3.2xlarge', #GPU best for our problem\n",
    "    framework_version=\"1.12.1\",\n",
    "    py_version=\"py38\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        'optimizer': 'Adam',\n",
    "        'epochs': 100,\n",
    "        'patience': 30,\n",
    "        'batch': selected_params['batch'], \n",
    "        'lr0': selected_params['lr0'],\n",
    "        'lrf': selected_params['lrf'],\n",
    "        'momentum': selected_params['momentum'],\n",
    "        'weight_decay': selected_params['weight_decay']\n",
    "    },\n",
    "    use_spot_instances=True,\n",
    "    # input_mode='File',  # FastFile causes a issue with writing label cache\n",
    "    debugger_hook_config=False,\n",
    "    max_wait=360000+3600,\n",
    "    max_run=360000,\n",
    "    output_path=output_path,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "# Run the training job with the selected hyperparameters\n",
    "estimator_train_final.fit({\"train\": train_data_path}, job_name=\"YOLO-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#62b6cb\"> 4. Generalization Check   <a name=\"id4\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and get the metric over the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and get the metric over the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
